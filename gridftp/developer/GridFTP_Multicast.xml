<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE appendix PUBLIC "-//OASIS//DTD DocBook XML V4.4//EN"
"http://www.oasis-open.org/docbook/xml/4.4/docbookx.dtd" [
<!ENTITY % xinclude SYSTEM "../../xinclude.mod">
%xinclude;
]>
<appendix id="gridftp-developer-multicast-driver">
  <title>GridFTP Multicast</title>

  <para>GridFTP is a well known, extremely fast and efficient protocol for
  transferring data from one destination to another. Here we present how
  GridFTP can be used to transfer a single file to many destinations in a
  multicast/broadcast.</para>

  <section>
    <title>Architecture</title>

    <para>The purpose of this work is to efficiently transfer a single data
    set to many locations. Our goal is to use all available network cycles,
    effectively moving the total amount of data (destinations * file size) at
    network speeds. Ideally, we would like to transfer to all destinations in
    the same time it takes to transfer to a single destination.</para>

    <para>Distributing the data to many endpoints is not difficult, and has
    been a feature of <command><olink
    targetptr="globus-url-copy">globus-url-copy</olink></command> (a popular
    GridFTP client) for some time. It would be quite simple to have the client
    loop over the destination set and send to each destination in series. Of
    course, this would be quite slow and the transfer time would scale
    linearly with the data set.</para>

    <para>In our architecture, destination servers are configured so that they
    can forward packets along to other endpoints. Thus, the GridFTP servers
    act as both servers receiving data and clients sending data. The server is
    set up as a tree such that the first destination writes the data to disk
    and then forwards it on to N more hosts (by default N is 2). This process
    is repeated until all destinations have received the data.</para>

    <para>For further explanation, the architecture can be thought of as a
    directed graph. Each destination is a vertex with N edges connecting it to
    N other vertices. A spanning tree is formed connecting all vertices. The
    degree of any one vertex is a client -side configuration option.</para>

    <para>The following image illustrates this:</para>

    <figure>
      <title>GridFTP Spanning Tree</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="mc.gif" />
        </imageobject>
      </mediaobject>
    </figure>

    <para>In the image, data blocks are purple and are sent first from the
    client to a root destination. The root destination then forwards it on to
    two more servers.</para>
  </section>

  <section>
    <title>Globus XIO</title>

    <para>The figure in the previous section shows 4 colored boxes. Orange
    represents client logic, blue is server logic, and as stated above, purple
    is for data block. The final box type is yellow and it is used to show
    globus XIO drivers.</para>

    <para>More information on Globus XIO can be found <olink
    targetdoc="xio">here</olink>. For our purposes we can think of each XIO
    driver as a modular protocol interpreter that can be plugged in to an IO
    stack without involving the application using it. In this way, we can add
    functionality to an existing application without disturbing its tested
    code base.</para>

    <para>Because the <command><olink
    targetptr="globus-gridftp-server">globus-gridftp-server</olink></command>
    uses Globus XIO for all of its IO, we are able to forward data at the
    block level. We achieve this by allowing the client to add a new XIO
    driver, the <computeroutput>gridftp_multicast</computeroutput> driver, to
    the GridFTP server's disk stack. Because of the modular driver abstraction
    that Globus XIO provides as the GridFTP server writes data blocks to its
    file system, the data blocks are first passed through the
    <computeroutput>gridftp_multicast</computeroutput> driver. As the
    <computeroutput>gridftp_multicast</computeroutput> driver passes the data
    block on to be written to disk, it also forwards the block on to other
    GridFTP servers in the tree.</para>

    <para>Using this approach to add the multicast functionality is minimally
    invasive to the tested and robust GridFTP server and is entirely modular.
    The driver is written to a well defined and clean abstraction. Enabling
    this feature is a simple matter of inserting the driver in the disk stack
    and passing the driver stack the destination list.</para>
  </section>

  <section id="gridftp-multicast-networkoverlay-arch">
    <title>Network Overlay</title>

    <para>In addition to allowing for multicast, the
    <computeroutput>gridftp_multicast</computeroutput> driver and this
    architecture allow us to create a network overlay where many GridFTP
    servers act as routers forwarding packets along to each other until they
    get to the final destination where there are written to disk. The
    advantage of this type of system is actively researched by <ulink
    url="http://e2epi.internet2.edu/phoebus.html">Phoebus</ulink>.</para>

    <para>The <computeroutput>gridftp_multicast</computeroutput> driver can be
    configured to only forward data along to the next server, and to not write
    it to disk. Furthermore, it can be told to only forward to a single
    endpoint. When configured in this way, we achieve the network overlay
    described above.</para>

    <para>The following images illustrate this. In the first image we show the
    standard case where data is sent from a client to a server through the
    Internet. The routing is done by the Internet outside of the clients
    control.</para>

    <figure>
      <title>From client to server through Internet</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="overlay1-s.png" />
        </imageobject>
      </mediaobject>
    </figure>

    <para>In the next image we show how the
    <computeroutput>gridftp_multicast</computeroutput> driver can route data
    through the network via GridFTP servers. This allows the user to have
    greater control over the network path which the data takes.</para>

    <figure>
      <title>Routing data through network via multicast</title>

      <mediaobject>
        <imageobject>
          <imagedata fileref="overlay2-s.png" />
        </imageobject>
      </mediaobject>
    </figure>
  </section>

  <section>
    <title>Results</title>

    <para>To show the effectiveness of this architecture we ran experiments on
    the <ulink url="http://www.uc.teragrid.org/tg-docs/">UC TeraGrid</ulink>.
    We show some of those results here.</para>

    <section>
      <title>Experiment 1</title>

      <para>In the first experiment, we leased nodes from the UC TeraGrid. 29
      hosts were designated as destinations and we ran
      <computeroutput>gridftp_multicast</computeroutput>-enabled GridFTP
      servers on them. 1 node was designated as the client node and from it
      all transfers were started. </para>

      <para>All transfers were performed with
      <command>globus-url-copy</command> and a tcp buffer size of 128KB. As a
      control group, we ran a transfer using
      <command>globus-url-copy</command> with the <option>-f</option> option.
      This caused the source file to be sent to each endpoint in serial. We
      then transferred the source to all destinations using this architecture.
      </para>

      <para>The first graph shows the completion time of a multicast session
      against the number of destinations. The first line is when the transfers
      are performed in a serial fashion from the client. All other lines are
      multicast sessions performed using this architecture. Each line
      represents a different vertex degree in the spanning tree (ie, each
      server forwarding to a different number of destinations).</para>

      <figure>
        <title></title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="complete_time.png" />
          </imageobject>
        </mediaobject>
      </figure>

      <para>As expected, the results for the serial transfer scales linearly
      while the multicast sessions very slowly increase with more
      destinations.</para>

      <para>The next graph shows the same experiment; but instead of graphing
      completion time, we graph the clients sending throughput. This is the
      size of the files being sent (1GB) divided by the time it takes for this
      file to reach all destinations.</para>

      <figure>
        <title></title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="broadcast_bw.png" />
          </imageobject>
        </mediaobject>
      </figure>

      <para>The final graph shows the collective bandwidth of a transfer. The
      graphing function is (# of destinations * file size) / time.</para>

      <figure>
        <title></title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="collective_bw.png" />
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section>
      <title>Future Experiments</title>

      <para>We created another XIO driver that allows each endpoint in the
      session to buffer the data. This prevents stalls in sending data
      transfers due to the latency required to reach a leaf node, and gets
      data to the disks of nodes higher in the tree faster. Early results show
      slight improvements on a LAN, but we expect greater results when
      broadcasting across WANs.</para>
    </section>
  </section>

  <section>
    <title>Protocol Details</title>

    <para>The additions to the protocol are exceptionally minor. Every server
    in the tree (except for leaf nodes) becomes a client to another server,
    but that client speaks the standard GridFTP protocol. The only change
    needed is a command to add the driver to the file system stack, and that
    command has existed in the GridFTP server for some time.</para>

    <para>The command is:</para>

    <screen>SITE SETDISKSTACK 1*{<replaceable>driver name</replaceable>[:<replaceable>driver options</replaceable>]},</screen>

    <para>The second parameter to the site command is a comma-separated list
    of driver names optionally followed by a colon (:) and a set of
    driver-specific URL-encoded options. From left to right, the driver names
    form a stack from bottom to top.</para>

    <para>Adding the <computeroutput>gridftp_multicast</computeroutput> driver
    to this list will enable the multicast functionality. The set of options
    are the same as those specified in the previous section. The only
    difference is that each url in the urls= options must be url
    encoded.</para>
  </section>

  <section>
    <title>Usage</title>

    <para>The broadcast functionality can be used with
    <command>globus-url-copy</command>. We added the following option:</para>

    <screen>-mc <replaceable>filename</replaceable></screen>

    <para>The file must contain a line separated list of destination urls. For
    example:</para>

    <screen>gsiftp://localhost:5000/home/user/tst1
gsiftp://localhost:5000/home/user/tst2
gsiftp://localhost:5000/home/user/tst4</screen>

    <para>The source url is specified on the command line as always. A single
    destination url may also be specified on the command line in addition to
    the urls in the file. An example globus-url-copy command is:</para>

    <screen>% globus-url-copy -MC multicast.file gsiftp://localhost/home/user/src_file</screen>

    <xi:include href="../user/index.xml"
                xmlns:xi="http://www.w3.org/2001/XInclude"
                xpointer="gridftp-user-experimental-multicasting"></xi:include>

<section><title>Required Server Options</title>
  <para>For security reasons the GridFTP server does not allow clients to load arbitrary xio drivers into the server. The GridFTP server admin must whitelist the driver individually. White-listing the mlink driver is done with the following parameter to the server:</para>

    <screen>-fs-whitelist file,gridftp_multicast</screen>

<para>Notice that <computeroutput>file</computeroutput> must also be specified. Without this option, the <computeroutput>file</computeroutput> driver is the default; however, if used, you must specifically list it.</para>
</section>
  </section>
</appendix>